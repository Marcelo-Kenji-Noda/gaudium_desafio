# ğŸš€ Desafio TÃ©cnico - Gaudium

O desafio consiste em aplicar modelagem dimensional a partir de uma tabela consolidada com informaÃ§Ãµes brutas, utilizando a abordagem de fato e dimensÃµes.

## ğŸ“ Estrutura dos dados

Arquivo de entrada: `Dados brutos.csv`
| Coluna       | Tipo |
|--------------|------|
| nome_cliente | str  |
| cidade       | str  |
| estado       | str  |
| nome_produto | str  |
| categoria    | str  |
| fabricante   | str  |
| data         | date |
| qtd_vendida  | int  |
| valor_total  | int  |

Durante a leitura do arquivo no PySpark, a opÃ§Ã£o `inferSchema=True` foi utilizada, permitindo a identificaÃ§Ã£o automÃ¡tica e correta dos tipos de dados.

## ğŸ“‹ IdentificaÃ§Ã£o de entidades e fatos

A partir de uma anÃ¡lise inicial, foram identificadas trÃªs principais dimensÃµes: **clientes**, **produtos** e **datas**. Os fatos estÃ£o associados Ã s medidas de quantidade vendida e valor total.

### Tabelas de DimensÃ£o: 

#### Cliente

A dimensÃ£o de clientes Ã© composta por nome, cidade e estado. A identificaÃ§Ã£o Ãºnica de cada cliente foi gerada por meio de um hash da combinaÃ§Ã£o desses trÃªs campos.

| Coluna       | Tipo |
|--------------|------|
| id_cliente   | int  |
| nome_cliente | str  |
| cidade       | str  |
| estado       | str  |

#### Produtos

A dimensÃ£o de produtos utiliza o nome do produto e o fabricante como identificadores Ãºnicos, tambÃ©m com o ID sendo um hash da combinaÃ§Ã£o desses campos.

| Coluna       | Tipo |
|--------------|------|
| id_produto   | int  |
| nome_produto | str  |
| categoria       | str  |
| fabricante       | str  |

#### Datas

A dimensÃ£o de datas foi construÃ­da com base no intervalo entre a primeira e a Ãºltima data de compra registrada. Ela inclui colunas adicionais Ãºteis para anÃ¡lises temporais.

| Coluna       | Tipo |
|--------------|------|
| data   | date  |
| dia | int  |
| mes       | int  |
|    ano    | int  |
|    dia_da_semana    | int  |

### A tabela fato criada:

#### Vendas fato

A tabela fato de vendas centraliza os dados relacionando as trÃªs dimensÃµes com as mÃ©tricas de interesse. A granularidade adotada Ã© por cliente, produto e data.

| Coluna       | Tipo |
|--------------|------|
| data   | date  |
| id_cliente | int  |
| id_produto       | int  |
|    qtde_vendida    | int  |
|    valor_total    | int  |

## ğŸ­ Diagrama do modelo e processo

O diagrama abaixo representa de forma simplificada o fluxo de transformaÃ§Ã£o dos dados neste projeto. Ele nÃ£o segue uma estrutura formal de diagramaÃ§Ã£o de processos ETL, pois o objetivo Ã© apenas ilustrar, de maneira clara e objetiva, as etapas especÃ­ficas adotadas neste caso.

![image](imgs/diagram.jpg)

## â–¶ï¸ Como executar

### CÃ³digo principal
Passos:

- Certifique-se de ter um ambiente com suporte ao PySpark.
- Coloque os arquivos `main.py` e config.toml no ambiente.
- FaÃ§a o upload do arquivo `.csv`. Por padrÃ£o, o script espera que o arquivo esteja no diretÃ³rio `data/raw/`, localizado no mesmo nÃ­vel do main.py. Esse caminho pode ser alterado no arquivo de configuraÃ§Ã£o (`config.toml`).
- Execute o arquivo `main.py`.

Os arquivos de saÃ­da serÃ£o gerados nos diretÃ³rios especificados no config.toml (por padrÃ£o, em `data/processed/`).

---

### CÃ³digo notebook (`notebooks/modelagem.ipynb`)

Essa versÃ£o foi utilizada para testar individualmente cada base de dados e pode ser Ãºtil para investigar etapas especÃ­ficas do processo de transformaÃ§Ã£o.

- Suba o notebook e o arquivo `.csv` em um ambiente com suporte ao pyspark
- Execute as cÃ©lulas sequencialmente

ObservaÃ§Ã£o:

Os caminhos dos arquivos estÃ£o escritos conforme a estrutura do repositÃ³rio. Para maior flexibilidade, esses caminhos podem ser modificados conforme necessÃ¡rio

## ğŸ›  Tecnologias utilizadas

- PySpark
- Spark SQL
- Databricks ou outra plataforma equivalente (Simulado via notebooks para facilitar a anÃ¡lise e testagem dos dados)
- Git e GitHub
- Figma (Utilizado na criaÃ§Ã£o dos diagramas que ilustram o modelo dimensional e o fluxo de transformaÃ§Ã£o dos dados.)

## ğŸ“‚ Arquivos no repositÃ³rio

A estrutura de arquivos do repositÃ³rio

- `imgs/*.jpg`
    - Imagens utilizadas no `README.md`
- `notebooks/modelagem.ipynb`
    - Notebook utilizado para gerar os dados individualmente. Recomendado principalmente para testar funÃ§Ãµes e transformaÃ§Ãµes especÃ­ficas em uma das tabelas
- `notebooks/testing.ipynb`
    - Notebook utilizado para realizar alguns testes de integridade dos dados gerados
- `main.py`
    - Script principal responsÃ¡vel pela orquestraÃ§Ã£o da geraÃ§Ã£o das tabelas a partir dos dados brutos.
- `config.toml`
    - Arquivo de configuraÃ§Ã£o do script `main.py`. Nele Ã© possÃ­vel:
        1. Alterar os caminhos de entrada e saÃ­da dos arquivos;
        2. Definir quais arquivos devem ser consolidados;
        3. Escolher quais colunas devem ser usadas como identificadores Ãºnicos.
- `requirements_dev.txt`
    - Lista de pacotes e suas versÃµes utilizados no desenvolvimento do projeto

### Arquivos gerados
Os arquivos de saÃ­da foram organizados para simular um processo de ETL. A estrutura segue o seguinte formato:

```
data
â”œâ”€â”€â”€processed
â”‚   â””â”€â”€ cliente
â”‚       â”œâ”€â”€ clientes_YYYYmmmdd.csv
â”‚       â””â”€â”€ clientes.csv
â”‚   â””â”€â”€ datas
â”‚       â”œâ”€â”€ datas_YYYYmmmdd.csv
â”‚       â””â”€â”€ datas.csv
â”‚   â””â”€â”€ produtos
â”‚       â”œâ”€â”€ produtos_YYYYmmmdd.csv
â”‚       â””â”€â”€ produtos.csv
â”‚   â””â”€â”€ vendas_fato.csv
â”‚       â”œâ”€â”€ vendas_fato_YYYYmmmdd.csv
â”‚       â””â”€â”€ vendas_fato.csv
â””â”€â”€â”€raw
    â””â”€â”€â”€ Dados brutos.csv
```

## âœ… ObservaÃ§Ãµes

- Para cada grupo de dados, sÃ£o gerados dois arquivos:
    - Uma versÃ£o com o nome fixo (ex: `clientes.csv`), que representa sempre a versÃ£o mais recente;
    - Uma versÃ£o com a data de geraÃ§Ã£o no nome (ex: `clientes_YYYYmmmdd.csv`), que pode ser utilizada para controle de versionamento.
- O cÃ³digo nÃ£o realiza leitura de tabelas previamente criadas. Isso significa que, ao adicionar um novo arquivo com dados, os arquivos de dimensÃµes e fato serÃ£o gerados apenas com base nesse novo arquivo, sem incorporar informaÃ§Ãµes de execuÃ§Ãµes anteriores.
- A utilizaÃ§Ã£o de hash para criaÃ§Ã£o das chaves nas tabelas de dimensÃµes foi adotada com o objetivo de facilitar uma futura implementaÃ§Ã£o mais robusta, em que o histÃ³rico e o controle de versionamento de dimensÃµes possam ser mantidos com consistÃªncia.